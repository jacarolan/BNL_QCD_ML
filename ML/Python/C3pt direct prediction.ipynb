{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from os import path\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRawVariables():\n",
    "    c2pt = []\n",
    "    ts   = []\n",
    "    taus = []\n",
    "    xs   = []\n",
    "    ys   = []\n",
    "    zs   = []\n",
    "    c3pt = []\n",
    "    \n",
    "    c2pt_start = 9\n",
    "    c3pt_start = 10155\n",
    "    \n",
    "    for tau in range(0, 49, 8):\n",
    "        for x in range(0, 25, 8):\n",
    "            for y in range(0, 25, 8):\n",
    "                for z in range(0, 25, 8):\n",
    "                    for sample in range(748, 1421, 16):\n",
    "                        fname = \"../Data/T\" + str(tau) + \"/x\" + str(x) + \"y\" + str(y) + \"z\" + str(z) + \"/nuc3pt.dat.\" + str(sample)\n",
    "                        if path.exists(fname):\n",
    "                            with open(fname) as fp:\n",
    "                                for i, line in enumerate(fp):\n",
    "                                    if i >= 7 and i <= 70:\n",
    "                                        c2pt.append([float(x) for x in line.rstrip().split()[1:3]])\n",
    "                                        ts.append(i - 7)\n",
    "                                        taus.append(tau)\n",
    "                                        xs.append(x)\n",
    "                                        ys.append(y)\n",
    "                                        zs.append(z)\n",
    "                                    elif i >= 10154 and i <= 10217:\n",
    "                                        c3pt.append([float(x) for x in line.rstrip().split()[1:3]])\n",
    "                                    elif i > 10217:\n",
    "                                        break\n",
    "    \n",
    "    return ts, taus, xs, ys, zs, c2pt, c3pt\n",
    "\n",
    "ts, taus, xs, ys, zs, c2pt, c3pt = LoadRawVariables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of c3pt based on c2pt directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([np.array([ts[i], taus[i], xs[i], ys[i], zs[i], c2pt[i][0], c2pt[i][1]]) for i in range(len(ts))])\n",
    "labels = np.array([np.array([c3pt[i][0], c3pt[i][1]]) for i in range(len(c3pt))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFrac = 0.9\n",
    "BCFrac = 0.05\n",
    "\n",
    "labelEnd = int(len(labels) * labelFrac)\n",
    "BCEnd    = int(len(labels) * (BCFrac + labelFrac))\n",
    "\n",
    "X_train, Y_train, X_bc, Y_bc, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:BCEnd], labels[labelEnd:BCEnd], features[BCEnd:], labels[BCEnd:]\n",
    "\n",
    "gbr_real = GradientBoostingRegressor(learning_rate=0.05, n_estimators=20, max_depth=3)\n",
    "gbr_real.fit(X_train, Y_train[:, 0])\n",
    "\n",
    "y_bc_pred_real = gbr_real.predict(X_bc)\n",
    "\n",
    "biasCrxn_real = np.average(Y_bc[:, 0] - y_bc_pred_real)\n",
    "\n",
    "gbr_imaginary = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "gbr_imaginary.fit(X_train, Y_train[:, 1])\n",
    "\n",
    "y_bc_pred_imaginary = gbr_imaginary.predict(X_bc)\n",
    "\n",
    "biasCrxn_imaginary = np.average(Y_bc[:, 1] - y_bc_pred_imaginary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48577280898.236565\n",
      "-69320103552.21849\n"
     ]
    }
   ],
   "source": [
    "print(biasCrxn_real)\n",
    "print(biasCrxn_imaginary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.75286102e+11]\n",
      "[9275451977509180.0]\n"
     ]
    }
   ],
   "source": [
    "trials = 0\n",
    "error = 0\n",
    "raw_RMS = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred_real = gbr_real.predict([testImg]) + biasCrxn_real\n",
    "    pred_imaginary = gbr_imaginary.predict([testImg]) + biasCrxn_imaginary\n",
    "    error += np.sqrt((pred_real - testLabel[0]) ** 2 + (pred_imaginary - testLabel[1]) ** 2)\n",
    "    raw_RMS += np.sqrt((testLabel[0]) ** 2 + (testLabel[1]) ** 2)\n",
    "    trials += 1\n",
    "error /= trials\n",
    "print(error)\n",
    "print([raw_RMS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of c3pt conglomerate based on c2pt snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4368 4368\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# features = np.array([np.array([ts[i], taus[i], xs[i], ys[i], zs[i], c2pt[i][0], c2pt[i][1]]) for i in range(len(ts))])\n",
    "features_unshifted = np.array([[taus[i]] + [c2pt[i + j][0] for j in range(64)] + [c2pt[i + j][1] for j in range(64)] for i in range(0, len(ts), 64)])\n",
    "features = []\n",
    "for f in features_unshifted:\n",
    "    shift = int(f[0])\n",
    "    features.append(np.roll(f[1:], -shift))\n",
    "\n",
    "#### THIS IS WORSE ---> Try sparsely sampling features to combat large amount of data \n",
    "# for i in range(len(features)):\n",
    "#     sparseSubFeature = np.array([features[i][j] for j in range(0, len(features[i]), 8)])\n",
    "#     features[i] = sparseSubFeature\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "labels = np.array([sum(c3pt[i:i+64][0]) / 64 for i in range(0, len(c3pt), 64)])\n",
    "\n",
    "print(len(features), len(labels))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFrac = 0.9\n",
    "BCFrac = 0.05\n",
    "\n",
    "labelEnd = int(len(labels) * labelFrac)\n",
    "BCEnd    = int(len(labels) * (BCFrac + labelFrac))\n",
    "\n",
    "X_train, Y_train, X_bc, Y_bc, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:BCEnd], labels[labelEnd:BCEnd], features[BCEnd:], labels[BCEnd:]\n",
    "\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.05, n_estimators=50, max_depth=3)\n",
    "gbr.fit(X_train, Y_train)\n",
    "\n",
    "y_bc_pred = gbr.predict(X_bc)\n",
    "\n",
    "biasCrxn = np.average(Y_bc - y_bc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9797405]\n"
     ]
    }
   ],
   "source": [
    "trials = 0\n",
    "error = 0\n",
    "raw_RMS = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred = gbr.predict([testImg]) + biasCrxn\n",
    "    error += abs(pred - testLabel)\n",
    "    trials += 1\n",
    "    \n",
    "error /= trials\n",
    "print(error / sum(abs(Y_test)) * len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train based on a NN rather than a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4368 4368\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# features = np.array([np.array([ts[i], taus[i], xs[i], ys[i], zs[i], c2pt[i][0], c2pt[i][1]]) for i in range(len(ts))])\n",
    "features_unshifted = np.array([[taus[i]] + [c2pt[i + j][0] for j in range(64)] + [c2pt[i + j][1] for j in range(64)] for i in range(0, len(ts), 64)])\n",
    "features = []\n",
    "for f in features_unshifted:\n",
    "    shift = int(f[0])\n",
    "    features.append(np.roll(f[1:], -shift))\n",
    "\n",
    "#### Try sparsely sampling features to combat large amount of data \n",
    "# for i in range(len(features)):\n",
    "#     sparseSubFeature = np.array([features[i][j] for j in range(0, len(features[i]), 8)])\n",
    "#     features[i] = sparseSubFeature\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "labels = np.array([sum(c3pt[i:i+64][0]) / 64 for i in range(0, len(c3pt), 64)])\n",
    "\n",
    "print(len(features), len(labels))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
       "             learning_rate_init=0.05, max_fun=15000, max_iter=1000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelFrac = 0.9\n",
    "\n",
    "labelEnd = int(len(labels) * labelFrac)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:], labels[labelEnd:]\n",
    "\n",
    "mlp = MLPRegressor(max_iter=1000, learning_rate=\"adaptive\", learning_rate_init=0.05)\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99228135]\n"
     ]
    }
   ],
   "source": [
    "trials = 0\n",
    "error = 0\n",
    "raw_RMS = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred = mlp.predict([testImg])\n",
    "    error += abs(pred - testLabel)\n",
    "    trials += 1\n",
    "    \n",
    "error /= trials\n",
    "print(error / sum(abs(Y_test)) * len(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

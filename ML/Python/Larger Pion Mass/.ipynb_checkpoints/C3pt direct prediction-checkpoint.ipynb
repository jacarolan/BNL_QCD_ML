{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from os import path\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRawVariables():\n",
    "    c2pt = []\n",
    "    ts   = []\n",
    "    taus = []\n",
    "    xs   = []\n",
    "    ys   = []\n",
    "    zs   = []\n",
    "    c3pt = []\n",
    "    \n",
    "    c2pt_start = 9\n",
    "    c3pt_start = 10155\n",
    "    \n",
    "    for tau in range(0, 49, 8):\n",
    "        for x in range(0, 25, 8):\n",
    "            for y in range(0, 25, 8):\n",
    "                for z in range(0, 25, 8):\n",
    "                    for sample in range(748, 1421, 16):\n",
    "                        fname = \"../Data/T\" + str(tau) + \"/x\" + str(x) + \"y\" + str(y) + \"z\" + str(z) + \"/nuc3pt.dat.\" + str(sample)\n",
    "                        if path.exists(fname):\n",
    "                            with open(fname) as fp:\n",
    "                                for i, line in enumerate(fp):\n",
    "#                                     if i >= 7 and i <= 70:           # The start of Gauss -> Point 2pt correlation functions\n",
    "                                    if i >= 5182 and i <= 5245:      # The start of Gauss -> Gauss 2pt correlation functions\n",
    "                                        c2pt.append([float(x) for x in line.rstrip().split()[1:3]])\n",
    "                                        ts.append(i - 5182)\n",
    "                                        taus.append(tau)\n",
    "                                        xs.append(x)\n",
    "                                        ys.append(y)\n",
    "                                        zs.append(z)\n",
    "                                    elif i >= 10154 and i <= 10217:\n",
    "                                        c3pt.append([float(x) for x in line.rstrip().split()[1:3]])\n",
    "                                    elif i > 10217:\n",
    "                                        break\n",
    "    \n",
    "    return ts, taus, xs, ys, zs, c2pt, c3pt\n",
    "\n",
    "ts, taus, xs, ys, zs, c2pt, c3pt = LoadRawVariables()\n",
    "\n",
    "c2pt_factor_raw = sum(np.array(c2pt)) / len(c2pt)\n",
    "N_factor = np.sqrt(c2pt_factor_raw[0] ** 2 + c2pt_factor_raw[1] ** 2)\n",
    "\n",
    "for i in range(len(c2pt)):\n",
    "    for j in range(2):\n",
    "        c2pt[i][j] /= N_factor\n",
    "        c3pt[i][j] /= N_factor\n",
    "c2pt = np.array(c2pt)\n",
    "c3pt = np.array(c3pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of c3pt based on c2pt directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw = np.array([np.append([ts[i], taus[i]], [c2pt[(i//64) * 64:(i//64+1) * 64, 0], c2pt[(i//64) * 64:(i//64+1) * 64, 1]]) for i in range(len(ts))])\n",
    "labels_raw = np.array([np.array([c3pt[i][0], c3pt[i][1]]) for i in range(len(c3pt))])\n",
    "\n",
    "s = np.arange(len(features_raw))\n",
    "np.random.shuffle(s)\n",
    "features = features_raw[s]\n",
    "labels = labels_raw[s]\n",
    "\n",
    "data_points = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFrac = 0.2\n",
    "BCFrac = 0.5\n",
    "\n",
    "labelEnd = (int(data_points * labelFrac) // 64) * 64\n",
    "BCEnd    = (int(data_points * (BCFrac + labelFrac)) // 64) * 64\n",
    "\n",
    "X_train, Y_train, X_bc, Y_bc, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:BCEnd], labels[labelEnd:BCEnd], features[BCEnd:], labels[BCEnd:]\n",
    "\n",
    "gbr_real = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "gbr_real.fit(X_train, Y_train[:, 0])\n",
    "\n",
    "y_bc_pred_real = gbr_real.predict(X_bc)\n",
    "\n",
    "biasCrxn_real = np.average(Y_bc[:, 0] - y_bc_pred_real)\n",
    "\n",
    "gbr_imaginary = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "gbr_imaginary.fit(X_train, Y_train[:, 1])\n",
    "\n",
    "y_bc_pred_imaginary = gbr_imaginary.predict(X_bc)\n",
    "\n",
    "biasCrxn_imaginary = np.average(Y_bc[:, 1] - y_bc_pred_imaginary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33378601211959946\n",
      "1.092777170822826\n"
     ]
    }
   ],
   "source": [
    "print(biasCrxn_real)\n",
    "print(biasCrxn_imaginary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning average:  [3.76541223]\n",
      "Data average:  24.666931630824486\n"
     ]
    }
   ],
   "source": [
    "trials = 0\n",
    "average_ML = 0\n",
    "average_Data = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred_real = gbr_real.predict([testImg]) + biasCrxn_real\n",
    "    pred_imaginary = gbr_imaginary.predict([testImg]) + biasCrxn_imaginary\n",
    "    if int(X_test[i][0]) == 9:# and int(X_test[i][1]):\n",
    "        average_ML += pred_real\n",
    "        average_Data += testLabel[0]\n",
    "        trials += 1\n",
    "print(\"Machine Learning average: \", average_ML / trials)\n",
    "print(\"Data average: \", average_Data / trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of c3pt conglomerate based on c2pt snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.array([np.array([ts[i], taus[i], xs[i], ys[i], zs[i], c2pt[i][0], c2pt[i][1]]) for i in range(len(ts))])\n",
    "features_unshifted = np.array([[taus[i]] + [c2pt[i + j][0] for j in range(64)] + [c2pt[i + j][1] for j in range(64)] for i in range(0, len(ts), 64)])\n",
    "features = []\n",
    "for f in features_unshifted:\n",
    "    shift = int(f[0])\n",
    "    features.append(np.roll(f[1:], -shift))\n",
    "\n",
    "#### THIS IS WORSE ---> Try sparsely sampling features to combat large amount of data \n",
    "# for i in range(len(features)):\n",
    "#     sparseSubFeature = np.array([features[i][j] for j in range(0, len(features[i]), 8)])\n",
    "#     features[i] = sparseSubFeature\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "labels = np.array([sum(c3pt[i:i+64][0]) / 64 for i in range(0, len(c3pt), 64)])\n",
    "\n",
    "print(len(features), len(labels))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFrac = 0.9\n",
    "BCFrac = 0.05\n",
    "\n",
    "labelEnd = int(len(labels) * labelFrac)\n",
    "BCEnd    = int(len(labels) * (BCFrac + labelFrac))\n",
    "\n",
    "X_train, Y_train, X_bc, Y_bc, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:BCEnd], labels[labelEnd:BCEnd], features[BCEnd:], labels[BCEnd:]\n",
    "\n",
    "gbr = GradientBoostingRegressor(learning_rate=0.05, n_estimators=50, max_depth=3)\n",
    "gbr.fit(X_train, Y_train)\n",
    "\n",
    "y_bc_pred = gbr.predict(X_bc)\n",
    "\n",
    "biasCrxn = np.average(Y_bc - y_bc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 0\n",
    "error = 0\n",
    "raw_RMS = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred = gbr.predict([testImg]) + biasCrxn\n",
    "    error += abs(pred - testLabel)\n",
    "    trials += 1\n",
    "    \n",
    "error /= trials\n",
    "print(error / sum(abs(Y_test)) * len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train based on a NN rather than a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.array([np.array([ts[i], taus[i], xs[i], ys[i], zs[i], c2pt[i][0], c2pt[i][1]]) for i in range(len(ts))])\n",
    "features_unshifted = np.array([[taus[i]] + [c2pt[i + j][0] for j in range(64)] + [c2pt[i + j][1] for j in range(64)] for i in range(0, len(ts), 64)])\n",
    "features = []\n",
    "for f in features_unshifted:\n",
    "    shift = int(f[0])\n",
    "    features.append(np.roll(f[1:], -shift))\n",
    "\n",
    "#### Try sparsely sampling features to combat large amount of data \n",
    "# for i in range(len(features)):\n",
    "#     sparseSubFeature = np.array([features[i][j] for j in range(0, len(features[i]), 8)])\n",
    "#     features[i] = sparseSubFeature\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "labels = np.array([sum(c3pt[i:i+64][0]) / 64 for i in range(0, len(c3pt), 64)])\n",
    "\n",
    "print(len(features), len(labels))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelFrac = 0.9\n",
    "\n",
    "labelEnd = int(len(labels) * labelFrac)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = features[:labelEnd], labels[:labelEnd], features[labelEnd:], labels[labelEnd:]\n",
    "\n",
    "mlp = MLPRegressor(max_iter=1000, learning_rate=\"adaptive\", learning_rate_init=0.05)\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 0\n",
    "error = 0\n",
    "raw_RMS = 0\n",
    "for i in range(len(X_test)):\n",
    "    testImg = X_test[i]\n",
    "    testLabel = Y_test[i]\n",
    "    pred = mlp.predict([testImg])\n",
    "    error += abs(pred - testLabel)\n",
    "    trials += 1\n",
    "    \n",
    "error /= trials\n",
    "print(error / sum(abs(Y_test)) * len(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
